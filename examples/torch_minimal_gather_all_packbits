import torch
import torch.distributed as dist
import os
import logging
import random
import numpy as np


def is_distributed() -> bool:
    return dist.is_available() and dist.is_initialized()


def setup_logger(rank: int) -> logging.Logger:
    logger = logging.getLogger(f"rank{rank}")
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - [Rank %(name)s] %(message)s')

    fh = logging.FileHandler(f"logs/log_rank{rank}.txt")
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    return logger


def bool_to_uint8(x: torch.Tensor) -> torch.Tensor:
    x = x.to(torch.uint8)
    pad_len = (8 - x.numel() % 8) % 8
    if pad_len:
        x = torch.cat([x, torch.zeros(pad_len, dtype=torch.uint8, device=x.device)])
    x = x.reshape(-1, 8)
    weights = torch.tensor([1,2,4,8,16,32,64,128], dtype=torch.uint8, device=x.device)
    return (x * weights).sum(dim=1)


def uint8_to_bool(x: torch.Tensor, num_bits: int) -> torch.Tensor:
    bits = ((x.unsqueeze(1) >> torch.arange(8, device=x.device)) & 1).to(torch.bool)
    return bits.flatten()[:num_bits]


def main():
    try:
        # Seguridad: verificar número de GPUs disponibles
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA no está disponible.")
        local_gpu_count = torch.cuda.device_count()
        if "WORLD_SIZE" in os.environ:
            world_size = int(os.environ["WORLD_SIZE"])
            if world_size > local_gpu_count:
                raise RuntimeError(f"Se requieren {world_size} GPUs pero solo hay {local_gpu_count}.")
        else:
            world_size = 1

        # Inicialización distribuida
        if "RANK" in os.environ:
            dist.init_process_group(backend="nccl", init_method="env://")
            rank = dist.get_rank()
        else:
            rank = 0  # modo no distribuido

        device = torch.device(f"cuda:{rank % local_gpu_count}")
        torch.cuda.set_device(device)

        # Configurar logger
        logger = setup_logger(rank)

        # Reproducibilidad
        torch.manual_seed(42 + rank)
        random.seed(42 + rank)
        np.random.seed(42 + rank)

        # Parámetros
        A, B = 2, 10
        num_bits = A * B

        # Tensor booleano aleatorio
        bool_matrix = torch.randint(0, 2, (A, B), dtype=torch.bool, device=device)
        logger.info(f"Matriz inicial:\n{bool_matrix.to(torch.uint8)}")

        # Codificación a uint8
        packed = bool_to_uint8(bool_matrix.flatten())

        # All-gather (si estamos en modo distribuido)
        if is_distributed():
            gathered = [torch.empty_like(packed) for _ in range(dist.get_world_size())]
            dist.all_gather(gathered, packed)
        else:
            gathered = [packed]  # Solo hay uno

        # Reconstrucción
        bool_list = [uint8_to_bool(p, num_bits).reshape(A, B) for p in gathered]
        result = torch.cat(bool_list, dim=0)

        logger.info(f"Matriz reconstruida:\n{result.to(torch.uint8)}")

        if is_distributed():
            dist.barrier()

    except Exception as e:
        print(f"[Rank {os.environ.get('RANK', 0)}] ERROR: {e}")
    finally:
        if is_distributed():
            dist.destroy_process_group()


if __name__ == "__main__":
    main()